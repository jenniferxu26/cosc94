{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get Data",
   "id": "de48700581668986"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:36.356387Z",
     "start_time": "2025-06-09T05:25:36.097837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re, datetime as dt"
   ],
   "id": "6611fe47adaa68a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:36.366391Z",
     "start_time": "2025-06-09T05:25:36.363391Z"
    }
   },
   "cell_type": "code",
   "source": "COLS = [\"post_id\", \"text\", \"timestamp\", \"lang\"]",
   "id": "ce12e5c9a173c2b9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:49.575292Z",
     "start_time": "2025-06-09T05:25:36.375687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## English\n",
    "# load and reformat\n",
    "df_en = (\n",
    "    pd.read_csv(\"data/raw/english_twitter.csv\",\n",
    "                usecols=[\"tweet_id\", \"body\", \"post_date\"],\n",
    "                dtype={\"tweet_id\": str})\n",
    "      .rename(columns={\"tweet_id\": \"post_id\",\n",
    "                       \"body\": \"text\"})\n",
    "      .assign(timestamp=lambda d:\n",
    "              pd.to_datetime(d[\"post_date\"], unit=\"s\", utc=True),\n",
    "              lang=\"en\")\n",
    "      .sort_values(\"timestamp\")\n",
    "      .drop_duplicates(subset=\"text\", keep=\"first\")\n",
    "      .loc[:, COLS]\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# drop duplicates and filter for S&P 500\n",
    "df_en = df_en.loc[df_en[\"text\"].str.contains(r'(?i)\\b(?:s\\s*&?\\s*p\\s*500|sp\\s*500)\\b', regex=True, na=False)]"
   ],
   "id": "5821398ef7aa1cc0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:49.743494Z",
     "start_time": "2025-06-09T05:25:49.739490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Rows:\", len(df_en))\n",
    "print(df_en)"
   ],
   "id": "42b3e04d6264e42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10458\n",
      "                     post_id  \\\n",
      "169       550559515515842561   \n",
      "191       550608100504260610   \n",
      "543       550806376092807168   \n",
      "1331      551058514500521985   \n",
      "3318      552111348130131969   \n",
      "...                      ...   \n",
      "3325409  1212055545554898946   \n",
      "3325544  1212067050409070595   \n",
      "3325624  1212077265997238274   \n",
      "3325888  1212108424575700993   \n",
      "3325998  1212117380236795910   \n",
      "\n",
      "                                                      text  \\\n",
      "169      Les leaders de 2014:- Dow Jones: $INTC +41%; $...   \n",
      "191      Weekly S&P500 #Stocks Performance $NBR $GM $QE...   \n",
      "543      perfectly trading the S&P 500 in 2014 $FB $MU ...   \n",
      "1331     S&P500 #Stocks Performance $VRTX $LH $DGX $NFL...   \n",
      "3318     Technology EPS Growth Will Beat S&P 500, Says ...   \n",
      "...                                                    ...   \n",
      "3325409       $TSLA to be included in #SP500 $SPY in 2020?   \n",
      "3325544  These 2 stocks dominated S&P 500 returns in 20...   \n",
      "3325624  These 2 stocks dominated S&P 500 returns in 20...   \n",
      "3325888  $ABMD $ALGN $AMZN NEW ARTICLE : 10 Best Perfor...   \n",
      "3325998  10 Best Performing S&P 500 Stocks Of The Decad...   \n",
      "\n",
      "                        timestamp lang  \n",
      "169     2015-01-01 07:49:52+00:00   en  \n",
      "191     2015-01-01 11:02:55+00:00   en  \n",
      "543     2015-01-02 00:10:48+00:00   en  \n",
      "1331    2015-01-02 16:52:42+00:00   en  \n",
      "3318    2015-01-05 14:36:17+00:00   en  \n",
      "...                           ...  ...  \n",
      "3325409 2019-12-31 16:58:55+00:00   en  \n",
      "3325544 2019-12-31 17:44:38+00:00   en  \n",
      "3325624 2019-12-31 18:25:14+00:00   en  \n",
      "3325888 2019-12-31 20:29:02+00:00   en  \n",
      "3325998 2019-12-31 21:04:38+00:00   en  \n",
      "\n",
      "[10458 rows x 4 columns]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.139500Z",
     "start_time": "2025-06-09T05:25:49.834336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Chinese\n",
    "# load and reformat\n",
    "cutoff = pd.Timestamp(\"2019-12-31 23:59:59\", tz=\"UTC\")\n",
    "df_cn = (\n",
    "    pd.read_csv(\"data/raw/chinese_guba.csv\",\n",
    "                dtype={\"id\": str},\n",
    "                on_bad_lines=\"skip\")\n",
    "      .rename(columns={\"Contents\": \"text\",\n",
    "                       \"Publish Time\": \"_ts\"})\n",
    "      # build the required columns\n",
    "      .assign(post_id = lambda _df: \"cn_\" + _df.index.astype(str),\n",
    "              timestamp = lambda _df: pd.to_datetime(_df[\"_ts\"],\n",
    "                                                         utc=True,\n",
    "                                                         errors=\"coerce\"),\n",
    "              lang          = \"zh\")\n",
    "      # drop rows after the cutoff date\n",
    "      .query(\"timestamp <= @cutoff\")\n",
    "      .dropna(subset=[\"timestamp\"])\n",
    "      .drop_duplicates(\"post_id\")\n",
    "      .loc[:, COLS]                 # keep only the desired columns\n",
    ")"
   ],
   "id": "14f908c98f833911",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.148981Z",
     "start_time": "2025-06-09T05:25:50.145703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Rows:\", len(df_cn))\n",
    "print(df_cn)"
   ],
   "id": "364a29cc92216a82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 21928\n",
      "        post_id                                               text  \\\n",
      "0          cn_0  15.5.5的暴跌终于来啦，人们似乎已然看懂啦这样的跌，却看不懂专家大媒们各类稀奇古怪驴唇不...   \n",
      "1          cn_1  数据及策略 2015年5月5日沪深300日先行指标为第一波快升后的回落；日技术指标为第一波快...   \n",
      "2          cn_2  牛市震荡要敢于高抛低吸 作者：荷叶 5月5日，沪深股市暴跌，沪深300跌3.99%，上证指数...   \n",
      "3          cn_3  5月5日，A股市场遭遇今年以来罕见的大幅回调。 从此三大股指期货表现来看，回调风险已经有所暴...   \n",
      "4          cn_4  达尔金融周二期指分析：三大期指集体下挫，现货走势弱于期货，中证500期指由于此前贴水较多跌幅...   \n",
      "...         ...                                                ...   \n",
      "21923  cn_21923  老唐复盘：说好的休息2合约，在机会年前还是忍不住，本周容许3次试仓：画线开空2次止损3个点，...   \n",
      "21924  cn_21924              北京房产，抗跌性极好，城市中心化，三四线乡镇化，做为一线的北京，，无可挑剔   \n",
      "21925  cn_21925  1，4090一线做空，止损4105，止盈40702，4070附近做多，止损 4055，止盈4...   \n",
      "21926  cn_21926                                        明天继续攻击的概率高。   \n",
      "21927  cn_21927  2020年沪深300指数一定会击穿3200点，打八折，唯一问题是上半年破还是下半年破，达到2...   \n",
      "\n",
      "                      timestamp lang  \n",
      "0     2015-05-05 20:13:21+00:00   zh  \n",
      "1     2015-05-06 07:15:31+00:00   zh  \n",
      "2     2015-05-06 08:04:48+00:00   zh  \n",
      "3     2015-05-06 08:41:08+00:00   zh  \n",
      "4     2015-05-06 11:29:19+00:00   zh  \n",
      "...                         ...  ...  \n",
      "21923 2019-12-31 11:38:27+00:00   zh  \n",
      "21924 2019-12-31 11:46:14+00:00   zh  \n",
      "21925 2019-12-31 14:35:51+00:00   zh  \n",
      "21926 2019-12-31 14:36:03+00:00   zh  \n",
      "21927 2019-12-31 15:21:54+00:00   zh  \n",
      "\n",
      "[21928 rows x 4 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.171361Z",
     "start_time": "2025-06-09T05:25:50.164400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### same chronological block\n",
    "overall_start = min(df_en[\"timestamp\"].min(), df_cn[\"timestamp\"].min())\n",
    "overall_end = max(df_en[\"timestamp\"].max(), df_cn[\"timestamp\"].max())\n",
    "print(f\"Start: {overall_start}, end: {overall_end}\")\n",
    "\n",
    "df_en[\"lang\"] = \"en\"\n",
    "df_cn[\"lang\"] = \"cn\"\n",
    "corpus = (pd.concat([df_en, df_cn], ignore_index=True)\n",
    "          .sort_values(\"timestamp\")\n",
    "          .reset_index(drop=True))\n",
    "print(f\"Length = {len(corpus)}\")"
   ],
   "id": "689940f438ed2b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2015-01-01 07:49:52+00:00, end: 2019-12-31 21:04:38+00:00\n",
      "Length = 32386\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "1442344001bb1171"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.693331Z",
     "start_time": "2025-06-09T05:25:50.196293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import time\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import jieba"
   ],
   "id": "83526e7da31b53d2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.785911Z",
     "start_time": "2025-06-09T05:25:50.703581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## align time\n",
    "# assign each post to the nearest date\n",
    "def align_date_noon(ts):\n",
    "    # if tz-aware, drop tz info so ts.time() works\n",
    "    if ts.tzinfo is not None:\n",
    "        ts = ts.tz_convert(None)\n",
    "    # compare to noon\n",
    "    if ts.time() <= time(12, 0, 0):\n",
    "        return ts.date()\n",
    "    else:\n",
    "        return (ts + pd.Timedelta(days=1)).date()\n",
    "\n",
    "for df in (df_en, df_cn):\n",
    "    # ensure datetime, convert UTC and drop tz\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_localize(None)\n",
    "    # apply the alignment\n",
    "    df['timestamp'] = df['timestamp'].apply(align_date_noon)"
   ],
   "id": "526ec2e5333d99d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize and Add Sentiment Labels",
   "id": "591efa3b66fac621"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.931430Z",
     "start_time": "2025-06-09T05:25:50.793209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## English: VADER + Loughran-McDonald\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Loughran-McDonald\n",
    "lm_df = pd.read_csv(\"data/raw/Loughran-McDonald_MasterDictionary_1993-2024.csv\")\n",
    "# words with \"Positive\" > 0\n",
    "lm_pos = set(lm_df.loc[lm_df['Positive'] > 0, 'Word'].str.lower())\n",
    "# words with \"Negative\" > 0\n",
    "lm_neg = set(lm_df.loc[lm_df['Negative'] > 0, 'Word'].str.lower())"
   ],
   "id": "4bfc72dc7689a160",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:50.945337Z",
     "start_time": "2025-06-09T05:25:50.942935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer(preserve_case=False)"
   ],
   "id": "6a0034bfcee19a7e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:52.855494Z",
     "start_time": "2025-06-09T05:25:50.958260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# combine sentiment function\n",
    "def en_sentiment(text):\n",
    "    # VADER score\n",
    "    vader_c = sid.polarity_scores(text)['compound']\n",
    "\n",
    "    # tokenize with TweetTokenizer\n",
    "    tokens = tt.tokenize(text)\n",
    "\n",
    "    # count lexicons\n",
    "    lm_s = sum(tok in lm_pos for tok in tokens) - sum(tok in lm_neg for tok in tokens)\n",
    "\n",
    "    # combine\n",
    "    raw = vader_c + lm_s\n",
    "    return 1 if raw > 0 else (-1 if raw < 0 else 0)\n",
    "\n",
    "df_en['label'] = df_en['text'].apply(en_sentiment)"
   ],
   "id": "b827194e465b0474",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:52.894670Z",
     "start_time": "2025-06-09T05:25:52.866499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Chinese: DLUT + BostonNLP\n",
    "dlut_df = (pd.read_csv(\"data/raw/dlut_emotions.csv\", dtype={\"id\": str}, on_bad_lines=\"skip\"))\n",
    "dlut_df.head()"
   ],
   "id": "1d93b937d3326c2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   词语   词性种类   词义数  词义序号  情感分类   强度   极性  辅助情感分类  强度.1  极性.1     .1  .2\n",
       "0  脏乱    adj   1.0   1.0    NN  7.0  2.0                               \n",
       "1  糟报    adj   1.0   1.0    NN  5.0  2.0                               \n",
       "2  早衰    adj   1.0   1.0    NE  5.0  2.0                               \n",
       "3  责备   verb   1.0   1.0    NN  5.0  2.0                               \n",
       "4  贼眼   noun   1.0   1.0    NN  5.0  2.0                               "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>词语</th>\n",
       "      <th>词性种类</th>\n",
       "      <th>词义数</th>\n",
       "      <th>词义序号</th>\n",
       "      <th>情感分类</th>\n",
       "      <th>强度</th>\n",
       "      <th>极性</th>\n",
       "      <th>辅助情感分类</th>\n",
       "      <th>强度.1</th>\n",
       "      <th>极性.1</th>\n",
       "      <th></th>\n",
       "      <th>.1</th>\n",
       "      <th>.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>脏乱</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>糟报</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早衰</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>责备</td>\n",
       "      <td>verb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>贼眼</td>\n",
       "      <td>noun</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:53.029715Z",
     "start_time": "2025-06-09T05:25:52.927622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 0 = word, 4 = sentiment category, 5 = intensity\n",
    "lex_df = (dlut_df.iloc[:, [0, 4, 5]].drop_duplicates())\n",
    "lex_df.columns = ['word', 'emotion', 'intensity']\n",
    "\n",
    "# P... = positive emotion\n",
    "# N... = negative emotion\n",
    "def signed_intensity(row):\n",
    "    emo = str(row['emotion']).strip()\n",
    "    inten = float(row['intensity'])\n",
    "\n",
    "    # if emotion starts with \"N\", treat intensity as negative\n",
    "    if emo.upper().startswith('N'):\n",
    "        return -inten\n",
    "    else:\n",
    "        return inten\n",
    "lex_df['signed_intensity'] = lex_df.apply(signed_intensity, axis=1)\n",
    "lex_df.head()"
   ],
   "id": "f7381560bb6a9a1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  word emotion  intensity  signed_intensity\n",
       "0   脏乱      NN        7.0              -7.0\n",
       "1   糟报      NN        5.0              -5.0\n",
       "2   早衰      NE        5.0              -5.0\n",
       "3   责备      NN        5.0              -5.0\n",
       "4   贼眼      NN        5.0              -5.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>signed_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>脏乱</td>\n",
       "      <td>NN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>糟报</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早衰</td>\n",
       "      <td>NE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>责备</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>贼眼</td>\n",
       "      <td>NN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:57.228842Z",
     "start_time": "2025-06-09T05:25:53.068454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dictionary: word -> signed intensity\n",
    "lex = dict(zip(lex_df['word'], lex_df['signed_intensity']))\n",
    "\n",
    "# sentiment function\n",
    "def cn_sentiment(text):\n",
    "    tokens = jieba.cut(str(text))\n",
    "    total_score = sum(lex.get(tok, 0) for tok in tokens)\n",
    "    if total_score > 0:\n",
    "        return 1\n",
    "    elif total_score < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_cn['label'] = df_cn['text'].apply(cn_sentiment)"
   ],
   "id": "bd08a555c9006ae5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\bb\\anaconda3\\Lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\bb\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.32999444007873535 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:25:57.313497Z",
     "start_time": "2025-06-09T05:25:57.286394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_en.to_csv(\"data/processed/df_en_processed.csv\", index=False)\n",
    "df_en"
   ],
   "id": "1e788246fe8b2603",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     post_id  \\\n",
       "169       550559515515842561   \n",
       "191       550608100504260610   \n",
       "543       550806376092807168   \n",
       "1331      551058514500521985   \n",
       "3318      552111348130131969   \n",
       "...                      ...   \n",
       "3325409  1212055545554898946   \n",
       "3325544  1212067050409070595   \n",
       "3325624  1212077265997238274   \n",
       "3325888  1212108424575700993   \n",
       "3325998  1212117380236795910   \n",
       "\n",
       "                                                      text   timestamp lang  \\\n",
       "169      Les leaders de 2014:- Dow Jones: $INTC +41%; $...  2015-01-01   en   \n",
       "191      Weekly S&P500 #Stocks Performance $NBR $GM $QE...  2015-01-01   en   \n",
       "543      perfectly trading the S&P 500 in 2014 $FB $MU ...  2015-01-02   en   \n",
       "1331     S&P500 #Stocks Performance $VRTX $LH $DGX $NFL...  2015-01-03   en   \n",
       "3318     Technology EPS Growth Will Beat S&P 500, Says ...  2015-01-06   en   \n",
       "...                                                    ...         ...  ...   \n",
       "3325409       $TSLA to be included in #SP500 $SPY in 2020?  2020-01-01   en   \n",
       "3325544  These 2 stocks dominated S&P 500 returns in 20...  2020-01-01   en   \n",
       "3325624  These 2 stocks dominated S&P 500 returns in 20...  2020-01-01   en   \n",
       "3325888  $ABMD $ALGN $AMZN NEW ARTICLE : 10 Best Perfor...  2020-01-01   en   \n",
       "3325998  10 Best Performing S&P 500 Stocks Of The Decad...  2020-01-01   en   \n",
       "\n",
       "         label  \n",
       "169          0  \n",
       "191          0  \n",
       "543          1  \n",
       "1331         0  \n",
       "3318         1  \n",
       "...        ...  \n",
       "3325409      0  \n",
       "3325544      0  \n",
       "3325624      0  \n",
       "3325888      1  \n",
       "3325998      1  \n",
       "\n",
       "[10458 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lang</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>550559515515842561</td>\n",
       "      <td>Les leaders de 2014:- Dow Jones: $INTC +41%; $...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>550608100504260610</td>\n",
       "      <td>Weekly S&amp;P500 #Stocks Performance $NBR $GM $QE...</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>550806376092807168</td>\n",
       "      <td>perfectly trading the S&amp;P 500 in 2014 $FB $MU ...</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>551058514500521985</td>\n",
       "      <td>S&amp;P500 #Stocks Performance $VRTX $LH $DGX $NFL...</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>552111348130131969</td>\n",
       "      <td>Technology EPS Growth Will Beat S&amp;P 500, Says ...</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325409</th>\n",
       "      <td>1212055545554898946</td>\n",
       "      <td>$TSLA to be included in #SP500 $SPY in 2020?</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325544</th>\n",
       "      <td>1212067050409070595</td>\n",
       "      <td>These 2 stocks dominated S&amp;P 500 returns in 20...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325624</th>\n",
       "      <td>1212077265997238274</td>\n",
       "      <td>These 2 stocks dominated S&amp;P 500 returns in 20...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325888</th>\n",
       "      <td>1212108424575700993</td>\n",
       "      <td>$ABMD $ALGN $AMZN NEW ARTICLE : 10 Best Perfor...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325998</th>\n",
       "      <td>1212117380236795910</td>\n",
       "      <td>10 Best Performing S&amp;P 500 Stocks Of The Decad...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10458 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T05:27:10.930143Z",
     "start_time": "2025-06-09T05:27:10.880242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_cn = df_cn.drop_duplicates(subset=\"text\")\n",
    "df_cn.to_csv(\"data/processed/df_cn_processed.csv\", index=False)\n",
    "df_cn"
   ],
   "id": "110c4f4f42ad3452",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        post_id                                               text  \\\n",
       "0          cn_0  15.5.5的暴跌终于来啦，人们似乎已然看懂啦这样的跌，却看不懂专家大媒们各类稀奇古怪驴唇不...   \n",
       "1          cn_1  数据及策略 2015年5月5日沪深300日先行指标为第一波快升后的回落；日技术指标为第一波快...   \n",
       "2          cn_2  牛市震荡要敢于高抛低吸 作者：荷叶 5月5日，沪深股市暴跌，沪深300跌3.99%，上证指数...   \n",
       "3          cn_3  5月5日，A股市场遭遇今年以来罕见的大幅回调。 从此三大股指期货表现来看，回调风险已经有所暴...   \n",
       "4          cn_4  达尔金融周二期指分析：三大期指集体下挫，现货走势弱于期货，中证500期指由于此前贴水较多跌幅...   \n",
       "...         ...                                                ...   \n",
       "21923  cn_21923  老唐复盘：说好的休息2合约，在机会年前还是忍不住，本周容许3次试仓：画线开空2次止损3个点，...   \n",
       "21924  cn_21924              北京房产，抗跌性极好，城市中心化，三四线乡镇化，做为一线的北京，，无可挑剔   \n",
       "21925  cn_21925  1，4090一线做空，止损4105，止盈40702，4070附近做多，止损 4055，止盈4...   \n",
       "21926  cn_21926                                        明天继续攻击的概率高。   \n",
       "21927  cn_21927  2020年沪深300指数一定会击穿3200点，打八折，唯一问题是上半年破还是下半年破，达到2...   \n",
       "\n",
       "        timestamp lang  label  \n",
       "0      2015-05-06   cn      1  \n",
       "1      2015-05-06   cn      1  \n",
       "2      2015-05-06   cn      1  \n",
       "3      2015-05-06   cn      1  \n",
       "4      2015-05-06   cn      1  \n",
       "...           ...  ...    ...  \n",
       "21923  2019-12-31   cn      0  \n",
       "21924  2019-12-31   cn      1  \n",
       "21925  2020-01-01   cn      0  \n",
       "21926  2020-01-01   cn     -1  \n",
       "21927  2020-01-01   cn      1  \n",
       "\n",
       "[20755 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lang</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cn_0</td>\n",
       "      <td>15.5.5的暴跌终于来啦，人们似乎已然看懂啦这样的跌，却看不懂专家大媒们各类稀奇古怪驴唇不...</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cn_1</td>\n",
       "      <td>数据及策略 2015年5月5日沪深300日先行指标为第一波快升后的回落；日技术指标为第一波快...</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cn_2</td>\n",
       "      <td>牛市震荡要敢于高抛低吸 作者：荷叶 5月5日，沪深股市暴跌，沪深300跌3.99%，上证指数...</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cn_3</td>\n",
       "      <td>5月5日，A股市场遭遇今年以来罕见的大幅回调。 从此三大股指期货表现来看，回调风险已经有所暴...</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cn_4</td>\n",
       "      <td>达尔金融周二期指分析：三大期指集体下挫，现货走势弱于期货，中证500期指由于此前贴水较多跌幅...</td>\n",
       "      <td>2015-05-06</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21923</th>\n",
       "      <td>cn_21923</td>\n",
       "      <td>老唐复盘：说好的休息2合约，在机会年前还是忍不住，本周容许3次试仓：画线开空2次止损3个点，...</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>cn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21924</th>\n",
       "      <td>cn_21924</td>\n",
       "      <td>北京房产，抗跌性极好，城市中心化，三四线乡镇化，做为一线的北京，，无可挑剔</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21925</th>\n",
       "      <td>cn_21925</td>\n",
       "      <td>1，4090一线做空，止损4105，止盈40702，4070附近做多，止损 4055，止盈4...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>cn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21926</th>\n",
       "      <td>cn_21926</td>\n",
       "      <td>明天继续攻击的概率高。</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>cn</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21927</th>\n",
       "      <td>cn_21927</td>\n",
       "      <td>2020年沪深300指数一定会击穿3200点，打八折，唯一问题是上半年破还是下半年破，达到2...</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>cn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20755 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
